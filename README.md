# Project-2

# Objective
The document classification solution should significantly reduce the manual human effort in the HRM .It should achieve a higher level of accuracy and automation with minimal human intervention.

# Textract
This package provides a single interface for extracting content from any type of file, without any irrelevant markup.
![image](https://user-images.githubusercontent.com/101926069/191774047-5e0cea88-1f36-4172-9ca8-67f21ba10a66.png)

# Converting all file to .txt (Process)
![image](https://user-images.githubusercontent.com/101926069/191774289-67b7fea7-fead-4feb-af81-bc8121864066.png)

# Merging all category files
![image](https://user-images.githubusercontent.com/101926069/191774534-c128a362-874c-4bf8-9186-f1ec4285a5fc.png)

# Converting text to .csv (Process)
![image](https://user-images.githubusercontent.com/101926069/191774826-14105151-31b1-48f9-bb1b-2cd2dd1b6c99.png)

# Data Set Details
After Extracting and Modifying the Dataset, given data contains a total of 4 Classes and 79 rows . 
![image](https://user-images.githubusercontent.com/101926069/191775189-00292a4d-f482-4362-a5a4-05ec070bce2a.png)
![image](https://user-images.githubusercontent.com/101926069/191775280-eeb2f7f6-802e-4b7f-a92a-19e61b157d43.png)

# Removing All Unwanted Character’s
Unwanted Character’s like - \n \t, http links, tags, hashtags, html tags, converting to lower case, removing white spaces etc.
Word Tokenization - Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.
Removing Stop-words - A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.
![image](https://user-images.githubusercontent.com/101926069/191775515-b54d836b-d38f-4859-bdab-3f733f643ce0.png)

# Text – Preprocessing
![image](https://user-images.githubusercontent.com/101926069/191775832-16baad2d-8502-48d1-9c59-915e549c174d.png)
![image](https://user-images.githubusercontent.com/101926069/191775880-10083804-432e-4878-bc34-84012665591c.png)
In natural language processing, text preprocessing is the practice of cleaning and preparing text data. NLTK and re are common Python libraries used to handle many text preprocessing tasks.
The Dataset contains 79 rows and 3 Column 
There are total four Classes – Peoplesoft, Workday, React JS Developer and SQL Developer.
![image](https://user-images.githubusercontent.com/101926069/191776257-afe1fb64-47d3-4941-8f1c-375aac5562ec.png)

# Labels
By using Pie chart roles applied feature is displayed to visualize the job roles.
![image](https://user-images.githubusercontent.com/101926069/191776750-5848dfb7-95d8-454c-8434-b2016dbb0aa9.png)
By using Histogram  roles applied feature is displayed to check the job types.
![image](https://user-images.githubusercontent.com/101926069/191777001-bafdc60d-1620-439a-815b-eeb90f4a3aaa.png)

# Word Cloud
![image](https://user-images.githubusercontent.com/101926069/191777149-34369ae3-95a7-4830-a102-44169c79e6de.png)

# TF-IDF
![image](https://user-images.githubusercontent.com/101926069/191777348-ec3e0eaf-e1d4-44b0-8e87-e4225b242c79.png)![image](https://user-images.githubusercontent.com/101926069/191777414-9d5bf23b-245b-465e-84e7-033e6b5b9c92.png)
TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words.We can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions.







